# üöÄ ETL Sales Pipeline with AWS Glue 5.0

![AWS Certified](https://img.shields.io/badge/AWS%20Certified-3x-232f3e?logo=amazonaws&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![AWS Glue](https://img.shields.io/badge/AWS%20Glue-ETL-orange?logo=amazonaws)
![Lambda](https://img.shields.io/badge/AWS%20Lambda-Serverless-ff9900?logo=amazonaws)
![Last Commit](https://img.shields.io/github/last-commit/cgarridolizana87/etl-procesamiento-ventas-glue)

Este proyecto implementa un job de AWS Glue 5.0 para transformar datos crudos de ventas (CSV en S3) en archivos Parquet estructurados. Aplica casteo seguro, validaci√≥n y c√°lculo de la m√©trica `total_venta`. Dise√±ado como base para pipelines anal√≠ticos modernos.

---

## üåê Idioma / Language

Este repositorio es biling√ºe. La documentaci√≥n est√° disponible en espa√±ol e ingl√©s.  
This repository is bilingual. Documentation is available in Spanish and English.

---

## üóÇÔ∏è Versiones del Proyecto

| Versi√≥n       | Descripci√≥n                                                                 |
|---------------|------------------------------------------------------------------------------|
| `v0-manual/`  | Versi√≥n local con PySpark, ejecutada manualmente                            |
| `v1-lambda/`  | Versi√≥n serverless con AWS Lambda y Glue como job automatizado              |
| `v2-airflow/` | (Pr√≥ximamente) Orquestaci√≥n con Apache Airflow y ejecuci√≥n programada       |

---

## ‚öôÔ∏è Tecnolog√≠as utilizadas

- AWS Glue 5.0 (Spark 3.5, Python 3.11)
- Amazon S3
- AWS Lambda
- PySpark
- Parquet
- (Preparado para Airflow, Redshift, QuickSight)

---

## üîÑ L√≥gica del ETL

**Entrada:**  
`s3://data-cgarridolizana/raw/ventas.csv`

**Transformaciones:**
- Cast `cantidad` ‚Üí `int`
- Cast `precio_unitario` ‚Üí `double`
- Calcular `total_venta = cantidad √ó precio_unitario`
- Filtrar registros inv√°lidos

**Salida:**  
`s3://data-cgarridolizana/processed/ventas/` como archivos Parquet

---

## üß™ Fragmento de c√≥digo

```python
args = getResolvedOptions(sys.argv, ["JOB_NAME"])
job.init(args["JOB_NAME"], args)

df = df.withColumn("total_venta", F.col("cantidad") * F.col("precio_unitario"))
df_clean.write.mode("overwrite").parquet("s3://.../processed/ventas/")
```

---

## üöÄ Pr√≥ximos pasos

- Activador autom√°tico con AWS Lambda
- Orquestaci√≥n con Apache Airflow
- Carga a Amazon Redshift
- Visualizaci√≥n con Power BI o QuickSight

---

## üôã‚Äç‚ôÇÔ∏è Autor

**Cristobal Garrido L.**  
AWS Data Engineer  
AWS Certified (3x): Cloud Practitioner, AI Practitioner, Solutions Architect Associate  
Python | AWS Glue | Lambda | Serverless ETL & Event-Driven Workflows